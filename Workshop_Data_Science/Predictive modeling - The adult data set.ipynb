{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modeling - An Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we apply machine learning methods to a real world data set, the _adult_ data set. The data set is available on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) and can be assessed and downloaded [here](https://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "\n",
    "For the purpose of this tutorial we already downloaded the data set. You may find it in the `data` folder (`./data/adult_data.txt`).\n",
    "\n",
    "Please note that this tutorial bases on a talk given by [Olivier Grisel](https://github.com/ogrisel) and [Tim Head](https://github.com/betatim) at [EuroScipy 2017](https://www.euroscipy.org/2017/). You can watch their tutorial on YouTube ([Part I](https://www.youtube.com/watch?v=Vs7tdobwj1k&index=3&list=PL55N1lsytpbekFTO5swVmbHPhw093wo0h) and [Part II](https://www.youtube.com/watch?v=0eYOhEF_aK0&list=PL55N1lsytpbekFTO5swVmbHPhw093wo0h&index=2)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The machine learning model development pipleline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./_img/ML_scheme.png\" style=\"height: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/adult_data.txt\"\n",
    "names = (\"age, workclass, fnlwgt, education, education-num, \"\n",
    "         \"marital-status, occupation, relationship, race, sex, \"\n",
    "         \"capital-gain, capital-loss, hours-per-week, \"\n",
    "         \"native-country, income\").split(', ')    \n",
    "data = pd.read_csv(filepath , names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first rows of the data set by calling the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good advice before staring with any type of data analysis: __Know your data!__\n",
    "\n",
    "Hence, we take a  look at the auxiliary data file `data_names.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! cat ./data/adult_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fnlwgt` stands for \"final weight\". This is used by the Census Bureau to create \"weighted tallies\" of any specified socio-economic characteristics of the population. As we are no Census experts and don't know how to properly use that information, we delete the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('fnlwgt', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __The goal as stated in the auxiliary file is to predict whether a person makes over 50K $ a year.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(column='education-num', bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(column='age', bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist('hours-per-week', bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('income')['income'].count().plot.bar(rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data['income'] == ' >50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_income = data[data['income'] == ' <=50K']\n",
    "high_income = data[data['income'] == ' >50K']\n",
    "\n",
    "bins = np.linspace(10, 90, 20)\n",
    "plt.hist(low_income['age'].values, bins=bins, alpha=0.5, label='<=50K')\n",
    "plt.hist(high_income['age'].values, bins=bins, alpha=0.5, label='>50K')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into `target` and `feature` data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['income']\n",
    "print(\"Target variable: \", target.shape)\n",
    "features_data = data.drop('income', axis=1)\n",
    "print(\"Features: \", features_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [column_name for column_name in features_data\n",
    "                    if features_data[column_name].dtype.kind in ('i', 'f')]\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = features_data[numeric_features]\n",
    "numeric_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = features_data.drop(numeric_features, axis=1)\n",
    "categorical_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data_encoded = categorical_data.apply(lambda x: pd.factorize(x)[0])\n",
    "categorical_data_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could have used use __one-hot encoding__ for categorical features calling `pd.get_dummies(features_data)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(features_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine numerical and categorical data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([numeric_data, categorical_data_encoded], axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.values.astype(np.float32)\n",
    "y = (target.values == ' >50K').astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set: \", X_train.shape)\n",
    "print(\"Test set: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Decision Trees__](https://en.wikipedia.org/wiki/Decision_tree_learning) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "* Simple to understand and to interpret (white box model). Trees can be visualized.\n",
    "* Requires little data preparation.\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called [overfitting](https://en.wikipedia.org/wiki/Overfitting). \n",
    "\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning parameters on the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), also known as an error matrix, is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table).\n",
    "\n",
    "<img src=\"./_img/ConfusionMatrix.png\" style=\"height: 400px;\">\n",
    "\n",
    "Source: [Harsha Pulletikurti](http://scaryscientist.blogspot.de/2016/03/confusion-matrix.html)\n",
    "\n",
    "An informative blog post on performance metrics was recently published by [Andrew Long](https://towardsdatascience.com/data-science-performance-metrics-for-everyone-4d68f4859eef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge:** Compute the accuracy of the model on the test set (the average number of times the model predictions in `y_pred` match the true labels in `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/accuracy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to __accuary__, __precison__, __sensistivy__ and __specisgty__ there are quite some more performance metrics (for an overview visit  [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)). One performance metric of particular interest is the [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve.\n",
    "\n",
    "The ROC curve is created by plotting the __true positive rate (TPR)__ (aka sensitivity) against the __false positive rate (FPR)__ (aka 1 − specificity) at various threshold settings. The ROC curve is generated by plotting the cumulative distribution function (__area under the probability distribution (AUC)__ from $-\\infty$ to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.\n",
    "\n",
    "\n",
    "In scikit-learn some classifiers offer the `predict_proba` method. This method is a (soft) classifier outputting the probability of the instance being in each of the classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "y_pred_proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot a __ROC curve__ we call the `roc_curve` function provided in the `sklearn.metrics` module. The __area under the curve (AUC)__ is computed by the `roc_auc_score` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = np.round(roc_auc_score(y_test, y_pred_proba),4)\n",
    "print(\"ROC AUC: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FPR and TPR\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# plot\n",
    "fix, ax = plt.subplots()\n",
    "ax.plot(fpr_dt, tpr_dt)\n",
    "ax.set_ylabel(\"True Positive Rate (TPR, sensitivity)\", size=14)\n",
    "ax.set_xlabel(\"False Positive Rate (FPR, 1-specificity)\", size=14)\n",
    "ax.set_title(\"Receiver Operating Characteristic Curve (AUC {})\".format(auc), size=18)\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-fold cross-validation (Source: [scikit-learn](http://scikit-learn.org/stable/modules/cross_validation.html))\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called __overfitting__. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test`, `y_test`. \n",
    "\n",
    "When evaluating different settings (__*hyperparameters*__) for estimators,there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called __“validation set”__: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called [__cross-validation__][1] (__CV__ for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called __k-fold CV__, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). \n",
    "\n",
    "The __performance measure reported by k-fold cross-validation is then the average of the values computed in the loop__. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small.\n",
    "\n",
    "\n",
    "![](./_img/K-fold_cross_validation_EN.png)\n",
    "\n",
    "Source: [Wikipedia][1]\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "results = cross_validate(clf, X_train, y_train, cv=5, scoring='roc_auc',\n",
    "                         return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Decision Tree (on validation folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['test_score']), \n",
    "    np.std(results['test_score'])))\n",
    "\n",
    "print(\"ROC AUC Decision Tree (on train folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['train_score']), \n",
    "    np.std(results['train_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge**: Try different values of `max_depth` such as: `1`, `2`, `5`, `10`,`15`,... Can you suggest an explanation for the impact of `max_depth` on the cross-validate score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm - Ensemble Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ensemble learning methods](https://en.wikipedia.org/wiki/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random forests](https://en.wikipedia.org/wiki/Random_forest) are an ensemble learning method for classification and regression, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests belong to the group of __averaging methods__. The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "In scikit-learn's [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) the ` n_estimators` controls the number of trees in the forest. The `max_features` parameter controls the size of the random subsets of features to consider when splitting a node. The `max_depth` parameter corresponds to the maximum depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, max_features=10,\n",
    "                             max_depth=10)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(clf, X_train, y_train, cv=5, scoring='roc_auc',\n",
    "                         n_jobs=-1, return_train_score=True)\n",
    "\n",
    "print(\"ROC AUC Random Forest (on validation folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['test_score']), \n",
    "    np.std(results['test_score'])))\n",
    "\n",
    "print(\"ROC AUC Random Forest (on train folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['train_score']), \n",
    "    np.std(results['train_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gradient Tree Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) (GBT) is a generalization of boosting to arbitrary differentiable loss functions. GBT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. \n",
    "\n",
    "The advantages of GBT are:\n",
    "\n",
    "* Natural handling of data of mixed type (= heterogeneous features)\n",
    "* Predictive power\n",
    "* Robustness to outliers in output space (via robust loss functions)\n",
    "\n",
    "In __boosting methods__, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "In scikit-learn's [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) implementation, the number of weak learners (i.e. regression trees) is controlled by the parameter `n_estimators`; The size of each tree can be controlled either by setting the tree depth via `max_depth` or by setting the number of leaf nodes via `max_leaf_nodes`. The `learning_rate` is a hyper-parameter in the range `(0.0, 1.0]` that controls overfitting via shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(max_leaf_nodes=5, learning_rate=0.1,\n",
    "                                 n_estimators=100)\n",
    "clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(clf, X_train, y_train, cv=5, scoring='roc_auc',\n",
    "                         n_jobs=-1, return_train_score=True)\n",
    "\n",
    "print(\"ROC AUC Gradient Boosted Trees (on validation folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['test_score']), \n",
    "    np.std(results['test_score'])))\n",
    "\n",
    "print(\"ROC AUC Gradient Boosted Trees (on train folds): {:.4f} +/-{:.4f}\".format(\n",
    "    np.mean(results['train_score']), \n",
    "    np.std(results['train_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison via model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we actually did not evaluate the model performance of the Random Forest Classifier nor the Gradient Boosting Classifier on a test set. Hence, we have to fit the model first using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may evaluate the model performance  on the test set. Once again we consider the [ROC AUC metric](https://de.wikipedia.org/wiki/Receiver_Operating_Characteristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC AUC for Gradient Boosting: {:0.4f}\".format(roc_auc_score(y_test, y_pred_proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute FPR and TPR for gradient boosting model\n",
    "fpr_gbm, tpr_gbm, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# plot\n",
    "fix, ax = plt.subplots()\n",
    "ax.plot(fpr_dt, tpr_dt, label=\"Decision Tree\")\n",
    "ax.plot(fpr_gbm, tpr_gbm, label=\"gmn\")\n",
    "ax.set_ylabel(\"True Positive Rate (TPR, sensitivity)\", size=14)\n",
    "ax.set_xlabel(\"False Positive Rate (FPR, 1-specificity)\", size=14)\n",
    "ax.set_title(\"Receiver Operating Characteristic Curve)\", size=18)\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=data['income'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering = np.argsort(clf.feature_importances_)[::-1]\n",
    "\n",
    "importances = clf.feature_importances_[ordering]\n",
    "feature_names = features.columns[ordering]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y = np.arange(len(feature_names))\n",
    "ax.barh(y,importances,align='center')\n",
    "ax.invert_yaxis() \n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(feature_names, size=14)\n",
    "ax.set_title(\"Variable importance\", size=18);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
