{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive modeling - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we apply techniques for [hyperparameter tuning][1] on a real world data set, the _adult_ data set. The data set is available on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) and can be assessed and downloaded [here](https://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "For the purpose of this tutorial we already downloaded the data set. You may find it in the `data` folder (`./data/adult_data.txt`).\n",
    "\n",
    "Please note that this tutorial bases on a talk given by [Olivier Grisel](https://github.com/ogrisel) and [Tim Head](https://github.com/betatim) at [EuroScipy 2017](https://www.euroscipy.org/2017/). You can watch their tutorial on YouTube ([Part I](https://www.youtube.com/watch?v=Vs7tdobwj1k&index=3&list=PL55N1lsytpbekFTO5swVmbHPhw093wo0h) and [Part II](https://www.youtube.com/watch?v=0eYOhEF_aK0&list=PL55N1lsytpbekFTO5swVmbHPhw093wo0h&index=2)).\n",
    "\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data/adult_data.txt\"\n",
    "names = (\"age, workclass, fnlwgt, education, education-num, \"\n",
    "         \"marital-status, occupation, relationship, race, sex, \"\n",
    "         \"capital-gain, capital-loss, hours-per-week, \"\n",
    "         \"native-country, income\").split(', ')    \n",
    "data = pd.read_csv(filepath , names=names)\n",
    "data = data.drop('fnlwgt', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first rows of the data set by calling the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __The goal is to predict whether a person makes over 50K $ a year.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into `target` and `feature` data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['income']\n",
    "features_data = data.drop('income', axis=1)\n",
    "features = pd.get_dummies(features_data)\n",
    "\n",
    "print(\"Target variable: \", target.shape)\n",
    "print(\"Features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.values.astype(np.float32)\n",
    "y = (target.values == ' >50K').astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set: \", X_train.shape)\n",
    "print(\"Validation set: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Decision Trees__](https://en.wikipedia.org/wiki/Decision_tree_learning) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "* Simple to understand and to interpret (white box model). Trees can be visualized.\n",
    "* Requires little data preparation.\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called [overfitting](https://en.wikipedia.org/wiki/Overfitting). \n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=8)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"ROC AUC Decision Tree: {:.4f} +/-{:.4f}\".format(\n",
    "      np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning your estimator\n",
    "\n",
    "Hyperparameters are not directly learned by the classifier or regressor from the data. They need setting from the outside. An example of a hyper-parameter is `max_depth` for a decision tree classifier. In `scikit-learn` you can spot them as the parameters that are passed to the constructor of your estimator.\n",
    "\n",
    "\n",
    "The best value of a hyper-parameter depends on the kind of problem you are solving:\n",
    "\n",
    "* how many features and samples do you have?\n",
    "* mostly numerical or mostly categorical features?\n",
    "* is it a regression or classification task?\n",
    "\n",
    "Therefore you should optimize the hyper-parameters for each problem, otherwise the performance of your classifier will not be as good as it could be.\n",
    "\n",
    "### Search over a grid of parameters\n",
    "\n",
    "This is the simplest strategy: you try every combination of values for each hyper-parameter. \n",
    "In scikit-learn __grid search__ is  provided by [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which exhaustively generates candidates from a grid of parameter values specified with the `param_grid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": [1, 2, 4, 8, 16, 32]}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, \n",
    "                           scoring='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created a `sklearn.model_selection._search.GridSearchCV` object we can access its attributes using the `.`-notation. For instance, the results of the cross-validation are stored in the `cv_results_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the values of `max_depth` and the average train and test scores for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, max_depth in enumerate(grid_search.cv_results_['param_max_depth']):\n",
    "    print(\"Max depth: {}, train score: {:.3f}, test score {:.3f}\".format(max_depth,\n",
    "          grid_search.cv_results_['mean_train_score'][n],\n",
    "          grid_search.cv_results_['mean_test_score'][n],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of model diagnostics we write a function, `plot_grid_scores`, which allows us to compare test and train performance at for each value of of a particular hyperparameter, such as `max_depth`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_scores(param_name, cv_result):\n",
    "    # access the parameter\n",
    "    param_values = np.array(cv_result[\"param_{}\".format(param_name)])\n",
    "    \n",
    "    # plotting\n",
    "    fix, ax = plt.subplots()\n",
    "\n",
    "    ax.set_title(\"Scores for {}\".format(param_name), size=18)\n",
    "    ax.grid()\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    \n",
    "    train_scores_mean = cv_result['mean_train_score']\n",
    "    test_scores_mean = cv_result['mean_test_score']\n",
    "    ax.scatter(param_values, train_scores_mean, s=80 ,marker='o', color=\"r\",\n",
    "                label=\"Training scores\")\n",
    "    ax.scatter(param_values, test_scores_mean, s=80, marker='o', color=\"g\",\n",
    "                label=\"Cross-validation scores\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    print(\"Best test score: {:.4f}\".format(np.max(test_scores_mean)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once implemented we can use the `plot_grid_scores` and apply it on the `grid_search.cv_results_` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"max_depth\", grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Challenge:** Extend the parameter grid to also search over different values for the `max_features` hyper-parameter. (Try: 3, 6, 12, 24, 48, and 96). Plot the results using the `plot_grid_scores` function from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/grid_search.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting information might be to lookt at the best three parameter combinations so far. We write a function called `report` to achieve tis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\\n\".format(results['params'][candidate]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random grid search\n",
    "\n",
    "An alternative to the exhaustive grid search is to sample parameter values at random. This has two main benefits over an exhaustive search:\n",
    "* A budget can be chosen independent of the number of parameters and possible values.\n",
    "* Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "[`RandomizedSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. In contrast to `GridSearchCV`, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by `n_iter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": sp_randint(1, 32),\n",
    "              \"max_features\": sp_randint(1, 96),\n",
    "             }\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_grid,\n",
    "                                   n_iter=36, scoring='roc_auc', return_train_score=True)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"max_depth\", random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same number of model evaluations you get a much better view of how the performance varies as a function of `max_depth`. This is a big advantage especially if one of the hyper-parameters does not influence the performance of the estimator. Though as you increase the number of dimensions making a projection into just one becomes more noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\": sp_randint(1, 32),\n",
    "              \"max_features\": sp_randint(1, 96),\n",
    "              \"min_samples_leaf\": sp_randint(15, 40)\n",
    "             }\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_grid,\n",
    "                                   n_iter=36, scoring='roc_auc', return_train_score=True)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"max_depth\", random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"max_features\", random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"min_samples_leaf\", random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may assess the best performing parameter combination using the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization\n",
    "\n",
    "Neither the exhaustive grid search nor random search adapt their search for the best hyper-parameter as they evaluate points. For the grid all points are chosen upfront, and for random search all of them are chosen at random.\n",
    "\n",
    "It makes sense to use the knowledge from the first few evaluations to decide what hyper-parameters to try next. This is what tools like [`scikit-optimize`](https://scikit-optimize.github.io/) try and do. The technique is known as Bayesian optimization or sequential model based optimization.\n",
    ".\n",
    "\n",
    "The basic algorithm goes like this:\n",
    "* evaluate a new set of hyper-parameters\n",
    "* fit a regression model to all sets of hyper-parameters\n",
    "* use the regression model to predict which set of hyper-parameters is the best\n",
    "* evaluate that set of hyper-parameters\n",
    "* repeat.\n",
    "\n",
    "`scikit-optimize` provides a drop-in replacement for `GridSearchCV` and `RandomSearchCV` that performs all this on the inside:\n",
    "\n",
    "_Note that if `scikit-optimize` is not yet installed on your machine type `conda install scikit-optimize` into your shell._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_search = BayesSearchCV(\n",
    "    clf,\n",
    "    {\"max_depth\": (1, 32),\n",
    "     \"max_features\": (1, 96),\n",
    "     \"min_samples_leaf\": (15, 40)\n",
    "    },\n",
    "    n_iter=15,\n",
    "    scoring='roc_auc',\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once the computation finished, we can access the results in the same fashion as we did before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_scores(\"max_depth\", bayes_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bayes_search.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cross validation results for predictions\n",
    "\n",
    "Once we finished our hyperparameter search, we may actually use the best model for predictions. Note that so far we did not build a test set, hence for the purpose of demonstration we use the validation set as test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.copy(X_val)\n",
    "y_test = np.copy(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use accuracy as our model evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is more than one way to make predictions for a hold out set (`X_test`). We may use the `best_estimator_` attribute to instantiate an estimator object, or use `predict` directly on the CV-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant 1\n",
    "m = bayes_search.best_estimator_\n",
    "y_pred_v1 = m.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy on the test set: \", accuracy_score(y_true=y_test, y_pred=y_pred_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant 2\n",
    "y_pred_v2 = bayes_search.predict(X_test)\n",
    "print(\"Accuracy on the test set: \", accuracy_score(y_true=y_test, y_pred=y_pred_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
