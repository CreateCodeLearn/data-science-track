{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) analysis is a statistical process for estimating the relationships between two or more variables. The relationship is modeled as \n",
    "\n",
    "$$y \\sim x$$\n",
    "or \n",
    "$$y = f(x)\\text{.}$$\n",
    "\n",
    "Both model descriptions indicate  that the variable $y$ is a function  of $x$. Therefore the variable $y$ is denoted as **response variable** or **dependent variable**, whereas the variable $x$ is denoted as **predictor variable** or **independent variable**. \n",
    "\n",
    "The function $f(x;\\beta)$ is called a **regression function** and $\\beta$ are its free parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/extrapolating.png\" width = \"500px\">\n",
    "\n",
    "Source: [xkcd](https://xkcd.com/605/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The machine learning model development process\n",
    "\n",
    "<img src=\"./_img/ML_scheme.png\" style=\"height: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we work with the *trees* data set, a data set that ships with the [R-package `datasets`](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/trees.html) (Ryan et al. 1976). The *trees* data set provides measurements of the girth, height and volume of timber in 31 felled black cherry trees, also known as [*Prunus serotina*](https://en.wikipedia.org/wiki/Prunus_serotina). The height of the trees is given in feet (ft) and the volume is given in in cubic feet (cft). The girth is the diameter of the tree (in inches) measured at 4 ft 6 in (approx. 1.37 m) above the ground.  \n",
    "\n",
    "We load the data set using pandas `read_csv` and take a quick look at the data set by applying the `head` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_raw = pd.read_csv(\"./data/trees.csv\", index_col=0)\n",
    "print(trees_raw.shape)\n",
    "trees_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding we convert the variables in meters, respectively, cubic meters, by applying these equations \n",
    "\n",
    "$$ 1\\; \\text{in} = 0.0254\\;\\text{m,}$$\n",
    "\n",
    "$$ 1\\; \\text{ft} = 0.3048\\;\\text{m,}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ 1\\; \\text{cft} = 0.0283168\\;\\text{m}^3\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to convert the features `Girth`,`Height` and `Volume` from inches, feet and cubic feet into meters and cubic meters and apply it on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2meter(s, input_unit=\"in\"):\n",
    "    '''\n",
    "    Function to convert inches, feet and cubic feet to meters and cubic meters \n",
    "    '''\n",
    "    if input_unit == \"in\":\n",
    "        return s*0.0254\n",
    "    elif input_unit == \"ft\":\n",
    "        return s*0.3048\n",
    "    elif input_unit == \"cft\":\n",
    "        return s*0.0283168\n",
    "    else:\n",
    "        print(\"Error: Input unit is unknown.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply function\n",
    "measurement_unit = {\"Girth\":\"in\",\n",
    "                    \"Height\":\"ft\", \n",
    "                    \"Volume\":\"cft\"}\n",
    "\n",
    "trees = trees_raw.copy()\n",
    "for feature in [\"Girth\", \"Height\", \"Volume\"]:\n",
    "    trees[feature] = trees_raw[feature].apply(lambda x: convert2meter(s=x,\n",
    "                                 input_unit=measurement_unit.get(feature))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to visualize the data we want a work with. However, instead of a scatter plot, which is fine for the comparison of two variables, we are going to plot a scatter plot matrix to account for multiple variables. The `seaborn` library provides the handy `pairplot` function for plotting scatter plot matrices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(trees);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A quick visual inspection indicates that there is an approximately linear relationship between girth and to somehow a less pronounced linear relationship between height and volume. Any relation between height and girth is less obvious. \n",
    "\n",
    "> The goal in this example is to build a linear regression model with `Volume` being the dependent variable and `Height` and `Girth` being the independent (explanatory) variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-(Validation)-Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to split our data set into two partitions: a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, ratio=0.3, rs=42):\n",
    "    np.random.seed(rs)\n",
    "    idx = np.random.choice([True, False], size=df.shape[0], \n",
    "                           replace=True, p=[1-ratio, ratio])\n",
    "    train = df.loc[idx,:]\n",
    "    test = df.loc[~idx,:]\n",
    "    print(\"Train set: {}\".format(train.shape))\n",
    "    print(\"Test set: {}\".format(test.shape))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(trees, ratio=0.4, rs=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "train.plot.scatter(x=\"Girth\", y=\"Volume\", ax=ax, \n",
    "                   alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "test.plot.scatter(x=\"Girth\", y=\"Volume\", ax=ax, \n",
    "                  alpha=0.5, s=60, color=\"g\", label=\"Test set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis and Model Parameters\n",
    "\n",
    "#### Simple Linear Regression \n",
    "\n",
    "\n",
    "In [**linear regression**](https://en.wikipedia.org/wiki/Simple_linear_regression) models a relationship between the response variable $y$ and and the predictor variable $x$ is given in form of a **linear** equation\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are constants and $\\epsilon$ is a **random error term** added to the linear model equation, thus resulting in a probabilistic model. The number $\\beta_0$ is called **intercept**, sometimes referred to as *bias, shift* or *offset*, and defines the point of intersection of the line  and the $y$-axis $(x=0)$. The number $\\beta_1$ is called **regression coefficient**. It is a measure of the slope of the **regression line**. Thus, $\\beta_1$ indicates how much the $y$-value changes when the $x$-value increases by 1 unit. The error term $\\epsilon_i$, sometimes referred to as *residuals*,  is assumed to consist of independent normal distributed values, $e_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "\n",
    "\n",
    "![Linear Model](./_img/lm1.png)\n",
    "\n",
    "\n",
    "The error $\\epsilon_i$ for each particular pair of values ($x_i,y_i$) is computed by the difference of the observed value $y_i$ and the predicted value given by $\\hat y_i$.\n",
    "\n",
    "$$\\epsilon_i = y_i - \\hat y_i$$\n",
    "\n",
    "Depending on the data $\\epsilon_i$ is a negative number if $y_i$ plots below the regression line or it is a positive number if $y_i$ plots above the regression line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Linear Regression \n",
    "\n",
    "A [multiple linear regression](https://en.wikipedia.org/wiki/Regression_analysis) model is a generalization of the simple linear regression model.\n",
    "\n",
    "\n",
    "By extending the equation $y = \\beta_0 + \\beta_1 x + \\epsilon$ to a set of $n$ observations and $d$ explanatory variables, $x_1,\\cdots, x_d$ the regression model can be written as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i & = \\beta_0 + \\sum_{j=1}^dx_{ij}\\beta_j + \\epsilon_i \\\\\n",
    "& = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} +\\cdots + \\beta_dx_{di} + \\epsilon_i \\text{.} \n",
    "\\quad i = 1,2,\\dots n\\text{, } x\\in \\mathbb R^d\\text{.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Matrix Notation\n",
    "\n",
    "A regression model based upon $n$ observations (measurements) consists of of $n$ response variables, $y_1, y_2,...y_n$. For the ease of notation we write the response variables as a one-dimensional column vector of the size $y_{n \\times 1}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf y_{n \\times 1}=  \n",
    "       \\begin{bmatrix}\n",
    "        y_{1} \\\\\n",
    "        y_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{n} \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Moreover, for each particular observation $x_i$ $(x_1, x_2,..., x_n)$ we represent the associated explanatory variables $d$ as a column vector as well.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf x_{i}=  \n",
    "       \\begin{bmatrix}\n",
    "        x_{i1} \\\\\n",
    "        x_{i2} \\\\\n",
    "        \\vdots \\\\\n",
    "        x_{id} \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "\\text{(e.g.)}  \\Rightarrow          \n",
    "  \\begin{bmatrix}\n",
    "  \\text{height} \\\\\n",
    "  \\text{weight} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\text{age} \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Further, by transposing $x_i$ we stack a set of $n$ observation vectors into a matrix $X$ of the form $x_{n \\times d}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\mathbf X_{n \\times d}= \n",
    "        \\begin{bmatrix}\n",
    "        -x_{1}^T - \\\\\n",
    "        -x_{2}^T -\\\\\n",
    "        \\vdots \\\\\n",
    "        -x_{n1}-\\\\\n",
    "        \\end{bmatrix}\n",
    "        = \n",
    "       \\begin{bmatrix}\n",
    "        x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "        x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_{n1} & x_{n2} & \\cdots & x_{nd} \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This matrix notation is very similar to a spreadsheet representation, where each row corresponds to an observation and each column to a feature. Please note that we assume that all features are continuous-valued $(x \\in \\mathbb R^d)$ and that there are more observations than dimensions $(n > d)$. \n",
    "\n",
    "\n",
    "For linear regression (and classification) the intercept term $\\beta_0$ does not interact with any element in the vector $x \\in \\mathbb R^d$. Thus it is convenient to populate the first column of the matrix $\\mathbf X$ with ones:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\mathbf X_{n \\times d+1}= \n",
    "       \\begin{bmatrix}\n",
    "        1 & x_{11} & x_{12} & \\cdots & x_{1d} \\\\\n",
    "        1 &x_{21} & x_{22} & \\cdots & x_{2d} \\\\\n",
    "        \\vdots &\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_{n1} & x_{n2} & \\cdots & x_{nd} \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the matrix $\\mathbf X$ is now of the size $n \\times d+1$ and that $\\beta\\in \\mathbb R^{d+1}$, and thus $[\\beta_0, \\beta_1,...,\\beta_d]^T$ is of size $d+1 \\times 1$. The vector $\\mathbf y$ is denoted as **target vector** and the matrix $\\mathbf X$ is called the **feature matrix**. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf y_{n \\times 1}=  \n",
    "       \\begin{bmatrix}\n",
    "        y_{1} \\\\\n",
    "        y_{2} \\\\\n",
    "        \\vdots \\\\\n",
    "        y_{n} \\\\\n",
    "        \\end{bmatrix}\\text{,}\n",
    "\\quad\n",
    "\\mathbf \\beta_{d+1 \\times 1}^T= \n",
    "       \\begin{bmatrix}\n",
    "        \\beta_0 \\\\\n",
    "        \\beta_1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_d  \\\\\n",
    "        \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Recalling the basics of [**Linear Algebra**](https://en.wikipedia.org/wiki/Linear_algebra), we know that the [*dot product*](https://en.wikipedia.org/wiki/Dot_product), also referred to as the *scalar product* of a vector $\\mathbf u_{1\\times n}$ with a vector $\\mathbf v_{n\\times 1}$ results in a scalar $w$ ($w$ may be written as a matrix of size $1\\times1$). The value of the scalar $w$ corresponds to the sum of the products of the elements in the vectors $\\mathbf u$ and $\\mathbf v$.\n",
    "\n",
    "$$\\mathbf u \\cdot \\mathbf v = \\sum_{i=1}^n  u_1v_1+u_2v_2+...+ u_nv_n$$\n",
    "\n",
    "![Data Layout](./_img/data-layout.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function (The loss function)\n",
    "\n",
    "Recall the equation for the multiple regression model\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\sum_{j=1}^dx_{ij}\\beta_j + \\epsilon_i  \\text{,} \n",
    "\\quad i = 1,2,...n\\text{, } x\\in \\mathbb R^d\n",
    "$$\n",
    "\n",
    "In order to find a appropriate set of parameters $\\beta$ we need to define an [**objective function**](https://en.wikipedia.org/wiki/Mathematical_optimization) $(\\mathcal L)$. \n",
    "\n",
    "One of the the most popular estimation method is the so-called [ordinary least squares (OSL) method](https://en.wikipedia.org/wiki/Ordinary_least_squares). Thereby the coefficients $\\beta_0, \\beta_1,\\beta_2,...,\\beta_{d}$ are picked by minimizing the residual sum of squares. \n",
    "\n",
    "$$\\mathcal L = \\sum_{i=1}^n\\epsilon_i^2=\\sum_{i=1}^n(y_i - f(x_i; \\beta))^2$$\n",
    "\n",
    "\n",
    "By plugging in the regression model equation from above we get \n",
    "\n",
    "\n",
    "$$\\mathcal L = \\sum_{i=1}^n(y_i - \\beta_0 + \\sum_{j=1}^dx_{ij}\\beta_j)^2\\text{,}$$\n",
    "where $n$ corresponds to the number of observations and $d$ corresponds to the number of features of our data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to construct the target vector, `y` and the feature matrix, `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_matrix(data, y_col, x_col):\n",
    "    if not isinstance(x_col, list):\n",
    "        x_col = [x_col]\n",
    "        \n",
    "    # target vector\n",
    "    y = data[y_col].values.reshape(-1,1)\n",
    "    print(\"Target vector y: {}\".format(y.shape))\n",
    "\n",
    "    # feature matrix\n",
    "    X = data.copy()\n",
    "    X[\"beta0\"]= 1\n",
    "    X = X[[\"beta0\"]+x_col].values\n",
    "    print(\"Feature matrix X: {}\".format(X.shape))    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = model_matrix(train, \n",
    "                                y_col=\"Volume\", \n",
    "                                x_col=[\"Height\", \"Girth\"])\n",
    "\n",
    "X_test, y_test = model_matrix(test, \n",
    "                              y_col=\"Volume\", \n",
    "                              x_col=[\"Height\", \"Girth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "\n",
    "Returning to our problem we see that we may rewrite the original least squares objective function\n",
    "\n",
    "$$\\mathcal L = \\sum_{i=1}^n(y_i - \\beta_0 + \\sum_{j=1}^dx_{ij}\\beta_j)^2\\text{,}$$\n",
    "\n",
    "by using the matrix notation as \n",
    "\n",
    "\n",
    "$$\\mathcal L = \\sum_{i=1}^n(y_i - x_i^T \\beta)^2$$\n",
    "\n",
    "The representation in matrix form allows us to further simplify the equation by plugging in the response vector $\\mathbf y$ and the model matrix $\\mathbf X$:\n",
    "\n",
    "\n",
    "$$\\mathcal L = \\sum_{i=1}^n(y_i - x_i^T \\beta)^2 = \\Vert\\mathbf y - \\mathbf X\\beta\\Vert^2 = (\\mathbf y - \\mathbf X\\beta)^T(\\mathbf y - \\mathbf X\\beta)$$\n",
    "Now, in order to find the least squares solution we take the gradient with respect to $\\beta$ and find a unique solution $\\hat \\beta$:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal L=2\\mathbf X^T\\mathbf X \\beta-2\\mathbf X^T\\mathbf y \\quad \\Rightarrow \\quad\\hat \\beta =  (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf y$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge:__ Implement the one step learning algorithm using `numpy`'s `dot` and `linalg.inv` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/one_step_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = one_step_learning(X_train, y_train)\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\beta_0 + \\beta_1x + \\beta_2x $$\n",
    "\n",
    "$$ \\text{Volume} = -1.70 + 0.04 \\times \\text{Height} + 5.05 \\times \\text{Girth}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "\n",
    "Given new data $\\mathbf X_{new}$, the least squares prediction for $\\hat y$ is\n",
    "\n",
    "$$\\hat y = \\mathbf X_{new}\\hat \\beta = \\mathbf X_{new}(\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf y$$\n",
    "\n",
    "\n",
    "Be aware that the calculation of $\\hat \\beta = (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf y$ assumes $(\\mathbf X^T\\mathbf X)^{-1}$ exists. Therefore $\\mathbf X^T\\mathbf X$ has to be a full rank matrix. A matrix is full rank when the $n \\times (d + 1)$ matrix $\\mathbf X$ has at least $d + 1$ linearly independent rows. This means that any point in $\\mathbb R^{d+1}$ can be reached by a weighted combination of $d + 1$ rows of $\\mathbf X$. Thus, if $n < d + 1$ least squares fails as if $(\\mathbf X^T\\mathbf X)^{-1}$ does not exist, as there are an infinite number of possible solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge:__ Write a function to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=22\n",
    "g=1.2\n",
    "predict(np.array([1, h, g]), beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = trees.sample(8, random_state=42).copy().reset_index(drop=True)\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set, _ = model_matrix(XX, \"Volume\", [\"Height\", \"Girth\"])\n",
    "prediction_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX.loc[:,\"Predicted Volume\"] = pd.Series(predict(prediction_set, beta_hat).ravel())\n",
    "XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions on the test set, the so far not seen data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test, beta_hat)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that out optimization approach is based on minimizing the **sum of squared errors (SSE)**:\n",
    "\n",
    "\n",
    "$$SSE = \\sum e^2 = \\sum (\\hat y - y)^2\\text{.}$$\n",
    "\n",
    "\n",
    "A widely applied metric to evaluate the goodness-of-fit of the model is the [**root-mean-square error (RMSE)**](https://en.wikipedia.org/wiki/Root-mean-square_deviation) defined by\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat y - y)^2}{n}}\\text{.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge:__ Write a function to compute the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/rmse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$\n",
    "\\begin{array}{lll}\n",
    "\\text{Summary} & \\\\\n",
    "\\hline\n",
    "\\text{Hypothesis} & \\text{Linear model} & \\bf{\\hat y} = \\bf x^T \\beta \\\\\n",
    "\\text{Model parameters} & \\beta_0, \\beta_1, \\beta_2 &\\\\\n",
    "\\text{Hyperparameters} & \\text{None}  &\\\\\n",
    "\\text{Objective function} & \\text{Sum of squared errors (SSE)} & \\sum (\\hat y - y)^2 \\\\\n",
    "\\text{Optimization} & \\text{One step learning (analytic approach)} & \\hat \\beta =  (\\mathbf X^T\\mathbf X)^{-1}\\mathbf X^T\\mathbf y \\\\\n",
    "\\text{Evaluation metric} & \\text{Root mean square error (RMSE)} &\\sqrt{\\frac{\\sum_{i=1}^n (\\hat y - y)^2}{n}} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
