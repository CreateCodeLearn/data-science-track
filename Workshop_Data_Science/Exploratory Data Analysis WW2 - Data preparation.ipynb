{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exploratory Data Analysis in Action - Data preparation\n",
    "\n",
    "In this notebook we apply some useful techniques for cleaning and organizing our data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100\n",
    "plt.rcParams[\"figure.figsize\"] = [15,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and organizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./_img/Time_data_science.png)\n",
    "\n",
    "Source: [Gil Press (2016)](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#55852a146f63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\"\n",
    "df_raw = pd.read_csv(PATH + \"operations.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "[Data cleansing or data cleaning](https://en.wikipedia.org/wiki/Data_cleansing) is the process of **detecting and correcting (or removing) corrupt or inaccurate records** from a record set, table, or database and refers to **identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open issues: Deal with incomplete, incorrect, inaccurate or irrelevant parts of the data\n",
    "\n",
    "-  Identify    \n",
    "-  Correct\n",
    "-  Spatial subsetting the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with incomplete data (`NaN`)\n",
    "\n",
    "Missing values in data sets are a well-known problem as nearly everywhere, where data is measured and recorded, issues with missing values occur. Various reasons lead to missing values: values may not be measured, values may be measured but get lost or values may be measured but are considered unusable. Missing values can lead to problems, because often further data processing and analysis steps rely on complete data sets. Therefore missing values need to be replaced with reasonable values. In statistics this process is called **imputation**.\n",
    "\n",
    "When faced with the problem of missing values it is important to understand the mechanism that causes missing data. Such an understanding is useful, as it may be employed as background knowledge for selecting an appropriate imputation strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for `NaN` **\n",
    "\n",
    "Note that in many cases missing values are assigned special characters, such as `-999`, `NA`, `k.A.` etc.; hence, you as a data analyst are responsible for taking appropriate action.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean.notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies to deal with missing data in Python**\n",
    "\n",
    "In general there are many options to consider when imputing missing values, for example:\n",
    "* A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "* A value from another randomly selected record.\n",
    "* A mean, median or mode value for the column.\n",
    "* A value estimated by another predictive model.\n",
    "\n",
    "There are some libraries implementing more or less advanced missing value imputation strategies such as \n",
    "\n",
    "* [`statsmodels`](http://www.statsmodels.org/dev/imputation.html) ([Multiple Imputation with Chained Equations (MICE)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/))\n",
    "* [`fancyimpute`](https://github.com/iskandr/fancyimpute) (matrix completion and imputation algorithms)\n",
    "* [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) (mean, median, most frequent)\n",
    "* [`pandas`](https://pandas.pydata.org/pandas-docs/stable/missing_data.html) ([`fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html), [`interpolate`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html) methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our working strategy to deal with missing data**\n",
    "\n",
    "_Owing to the fact that the amount of missing values in our data set is considerable high and that probably our domain knowledge with respect to World War II history is limited, we simply remove all features of the data set where more the 50% of the data is missing._  \n",
    "> **Challenge**: Write a function (or a script) which takes in our data set and a threshold value of 0.5 and which returns the column names of features that include more data than the given threshold. We then use the returned column names for subsetting our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_above_threshold(df, threshold=0.5):\n",
    "    ## your code here ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/features_above_threshold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply `features_above_threshold` on our data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2keep = features_above_threshold(df_clean, threshold=0.5)\n",
    "cols2keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reassign subsetted data set to the variable `df_clean`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[cols2keep]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with incorrect data (looking for outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Check that mission dates are between 1939 and 1945**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Mission Date\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[0:15, \"Mission Date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Challenge**: Reassign the `Mission Date` variable, a `string` object with o a time-aware `datetime` object.    \n",
    "_Hint: pandas comes with a_ `.to_datetime` _method_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/infere_mission_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Mission Date\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a `datetime` object we can use it to extract the year, month and day assign those to a unique columns in out data set (`year`, `month` and `day`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"year\"] = df_clean[\"Mission Date\"].dt.year\n",
    "df_clean[\"month\"] = df_clean[\"Mission Date\"].dt.month\n",
    "df_clean[\"day\"] = df_clean[\"Mission Date\"].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[[\"Mission Date\", \"year\", \"month\", \"day\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we ask for the minimum and the maximum value of the `year` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean[\"year\"].min())\n",
    "print(df_clean[\"year\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Check if the values in the altitude column (`Altitude (Hundreds of Feet)`) is within reasonable bounds__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Add a new column to the data set (`Altitude (meters)`) by transforming th values in the `Altitude (Hundreds of Feet)` from feet to meters.   \n",
    "_Hint: You may use the `apply` method._\n",
    "\n",
    "$$1\\;\\text{foot} =  0.3048\\; \\text{meter}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ...\n",
    "df_clean[\"Altitude (meters)\"] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/altitude_in_meters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Altitude (meters)\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"Altitude (meters)\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the output from above and answers to the question \"_At what altitude could WW2 planes fly?_\" on [Quora](https://www.quora.com/At-what-altitude-could-WW2-planes-fly-What-was-the-record-altitude-during-the-war-and-what-planes-could-fly-at-a-high-altitude-Could-they-fly-at-the-same-altitude-as-these-days-or-not), indicating that flight heights above 40,000 feet (approx. 12,000 m) occurred rarely.\n",
    "\n",
    "> __Challenge__: Limit the reported altitudes to 15,000 m and replace  outliers with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_height = 15000\n",
    "print((df_clean[\"Altitude (meters)\"] > max_height).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/limit_altitude.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check replacement\n",
    "print((df_clean[\"Altitude (meters)\"] > max_height).sum())\n",
    "\n",
    "df_clean[\"Altitude (meters)\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Check if the values in the target location columns (`Target Latitude` and `Target Longitude`) is within reasonable bounds__.\n",
    "\n",
    "Note that the location is given in geographic coordinates, hence, the values should be within -90 to 90 degree latitude and -180 to 180 degree longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['Target Latitude', 'Target Longitude']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clean.plot.scatter( x='Target Longitude', y='Target Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the table and the plot above there are some deviations from the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Replace geographical coordinates outside the natural limits [-90, 90] degrees latitude and [-180, 180] degrees longitude with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./src/_solutions/limit_coordinates.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "df_clean.plot.scatter( x='Target Longitude', y='Target Latitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with irrelevant data\n",
    "\n",
    "Our data set, as well as other data sets we encounter, sometimes come with features which are irrelevant for the purpose our research question. Hence, to reduce the data set size for the purpose of better readability as well as memory issues, among other, we may drop data columns. However, once again domain knowledge helps to decide which columns are of interest or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data set above we decide to drop the columns `Theater of Operations`, `Air Force`, `Target ID`, `Target Priority`, `Altitude (Hundreds of Feet)`, `Source ID`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2exlude = ['Theater of Operations', 'Air Force', 'Target ID', \n",
    "               'Target Priority', 'Altitude (Hundreds of Feet)', 'Source ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to achieve this task with Python is to use the powerful [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions).  \n",
    "\n",
    "    df_clean = df_clean[[c for c in df_clean.columns if c not in cols2exlude]] \n",
    " \n",
    "Another way is to leverage the pandas functionality and use the `drop` method. Note that for the purpose of memory and computation efficiency in many cases pandas returns a view of the object, rather than a copy. Hence, if to make a permanent change we have to assign/reassign the object to a variable:\n",
    "\n",
    "    df_clean = df_clean.drop(cols2exlude, axis=1)\n",
    "\n",
    "or use the `inplace=True` argument:\n",
    "\n",
    "    df_clean.drop(cols2exlude, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(cols2exlude, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check operation\n",
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial subsetting the data set\n",
    "\n",
    "_Note: In the subsequent cells we load Python library for spatial data analysis, such as `shapely`, `fiona`,`geopandas`, `cartopy` and `folium`. Make sure that you have installed the [GDAL bindings](http://www.gdal.org/index.html) on your computer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Challenge__: Install the modules `geopandas`, `cartopy` and `folium` into your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you may have already noticed that the data set contains data of aerial bombing during World War II for all araound the world. In this tutorial however, we want to focus on Europe. Hence, we are going to subset the data set accordingly.\n",
    "\n",
    "Owing to the manifold of spatial data representation, working with spatial data becomes sometimes more involved. In this section we make use third party libraries, such as [GeoPandas](http://geopandas.org/index.html) and [shapely](http://toblerity.org/shapely/), which abstract away many algorithmic or computational issues related to spatial data processing and plotting by integrating the workhorses of geospatial computing, such as [GEOS](http://trac.osgeo.org/geos/), [GDAL](http://www.gdal.org/), [OGR](http://gdal.org/1.11/ogr/) and [proj.4](http://proj4.org/), among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the variables `Target Latitude` and `Target Longitude` to spatial coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[['Target Latitude', 'Target Longitude']].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "geometry = [Point(xy) for xy in zip(df_subset['Target Longitude'], df_subset['Target Latitude'])]\n",
    "geometry[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the GeoPandas to make a pandas `DataFrame` spatially aware.**\n",
    "\n",
    "\n",
    "[GeoPandas](http://geopandas.org/index.html) extends the datatypes used by pandas to allow spatial operations on geometric types. Geometric operations are performed by [shapely](http://toblerity.org/shapely/). GeoPandas further depends on [fiona](http://toblerity.org/fiona/README.html) for file access and descartes and [matplotlib](https://matplotlib.org/) for plotting.\n",
    "\n",
    "It combines the capabilities of pandas and shapely, providing geospatial operations in pandas and a high-level interface to multiple geometries to shapely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf = gpd.GeoDataFrame(df_subset, geometry=geometry)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure that for every entry we have a valid spatial coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.shape)\n",
    "# subset only vaild spatial coordinates\n",
    "gdf = gdf.loc[gdf[['Target Longitude', 'Target Latitude']].notnull().all(axis = 1)]\n",
    "print(gdf.shape)\n",
    "gdf[['Target Longitude', 'Target Latitude']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign a spatial coordinate reference system (`crs`) to our GeoPandas object**\n",
    "\n",
    "\n",
    "In general the CRS may be defined in several ways, for example the CRS may be defined as [Well-known text (WKT)](https://en.wikipedia.org/wiki/Well-known_text) format, or [JSON](https://en.wikipedia.org/wiki/JSON) format, or [GML](https://en.wikipedia.org/wiki/Geography_Markup_Language) format, or in the [Proj4](https://en.wikipedia.org/wiki/PROJ.4) format, among many others.\n",
    "\n",
    "The Proj4 format is a generic, string-based description of a CRS. It defines projection types and parameter values for particular projections. For instance the Proj4 format string for the [European Terrestrial Reference System 1989 (ETRS89)](https://en.wikipedia.org/wiki/European_Terrestrial_Reference_System_1989) is:\n",
    "\n",
    "    +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no\\_defs\n",
    "\n",
    "\n",
    "With respect to the enormous amount of existing CRS the [International Association of Oil & Gas Producers (IOGP)](https://en.wikipedia.org/wiki/International_Association_of_Oil_%26_Gas_Producers), formerly known as **_European Petroleum Survey Group (EPSG)_**, built a collection of definitions for global, regional, national and local coordinate reference systems and coordinate transformations, the [EPSG Geodetic Parameter Dataset](http://www.epsg.org/). Within this collection each particular coordinate reference systems gets an unique integer identifier, commonly denoted as EPSG. For instance, the EPSG identifier for the the latest revision of the [World Geodetic System (WGS84)](https://en.wikipedia.org/wiki/World_Geodetic_System) is simply [4326](http://spatialreference.org/ref/epsg/4326/).\n",
    "\n",
    "\n",
    "A nice look up page for different coordinate reference systems is found [here](https://epsg.io/) and a fancy visualization of many prominent map projections is found [here](https://bl.ocks.org/mbostock/raw/3711652/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context matters: Load _Natural Earth countries_ dataset, bundled with GeoPandas\n",
    "\n",
    "[Natural Earth](http://www.naturalearthdata.com/) is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software. A subset comes bundled with GeoPandas and is accessible from the `gpd.datasets` module. We’ll use it as a helpful global base layer map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.plot(facecolor='lightgray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine world map and the Aerial Bombing data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = world.plot(facecolor='lightgray')\n",
    "gdf.plot(ax=base, marker='o', color='red', markersize=10, alpha=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated above we want to focus on Europe. For that we first want to know how many times the targets in our data set we in Germany, France or Italy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = (int(np.round(gdf[\"Target Country\"].\n",
    "                        isin([\"GERMANY\", \"FRANCE\", \"ITALY\"]).sum()/gdf.shape[0] * 100)))\n",
    "\n",
    "print(\"Approximately {}% of the targets in our data set are located in Germany, France or Italy.\".format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we want to restrict our analysis to data points, which refer to an area within Europe. Although it is not straightforward to define Europe as an entity, in terms of geography, politics or sphere of cultural identity, we define Europe as an area between the coordinates  \n",
    "\n",
    "$$\\text{33.0 to 73.5°N and 27.0°W to 45.0°E.}$$\n",
    "\n",
    "In order to represent that area spatially; we construct a `Polygon` object to represent the [bounding box](https://en.wikipedia.org/wiki/Minimum_bounding_box) of Europe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "# generate geopandas object\n",
    "poly_europe = gpd.GeoSeries([Polygon([(-27,33), (45,33), (45,73.5), (-27,73.5)])])\n",
    "bb_europe = gpd.GeoDataFrame({'geometry': poly_europe})\n",
    "# assign crs\n",
    "bb_europe.crs = {'init':'epsg:4326'}\n",
    "bb_europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot world map and bounding box of Europe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=world.plot(facecolor='lightgray')\n",
    "bb_europe.plot(ax=base, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset (intersect) the GeoPandas `DataFrame` with the bounding box of Europe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe = gpd.sjoin(gdf, bb_europe, how=\"inner\", op='intersects').drop(\"index_right\", axis=1)\n",
    "print(gdf_europe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_europe.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to keep the spatial context we extract the area of Europe from the world map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe = gpd.overlay(world, bb_europe, how='intersection')\n",
    "europe.crs = {'init': 'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base = europe.plot(facecolor='lightgray')\n",
    "gdf_europe.plot(ax=base, marker='o', color='red', markersize=5, alpha=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready!\n",
    "\n",
    "So now we have everything we need, a cleaned and subsetted data set `gdf_europe` and a spatial representation of Europe, `europe`, both in form of GeoPandas `DataFrame` objects.\n",
    "\n",
    "\n",
    "For further usage we write the GeoPandas `GeoDataFrame` objects to disk. `GeoDataFrames` can be exported to many different standard formats using the `GeoDataFrame.to_file()` method. Howerver, for the purpose of this tutorial we serialize the data by applying the [`pickle` module](https://docs.python.org/3/library/pickle.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(gdf_europe, open(\"./data/gdf_europe.p\", \"wb\"))\n",
    "pickle.dump(europe, open(\"./data/europe.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Advanced spatial plotting using cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "sys.path.append(src_dir)\n",
    "print(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.cuteplot(gpd_df=gdf_europe, crs=\"Orthographic\", title=\"Map projection: Orthographic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following loop may take some time, as each plot consists of 100,000+ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for proj in [\"Mollweide\", \"Robinson\", \"PlateCarree\", \"UTM32N\"]:\n",
    "    hf.cuteplot(gpd_df=gdf_europe, crs=proj, title=\"Map projection: \" + proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get more intuition about map projections and its effects on area, shape and angles check out [Tissot's indicatrix](https://en.wikipedia.org/wiki/Tissot%27s_indicatrix). By the way there is a nice implementation of Tissot's indicatrix in [cartopy](http://scitools.org.uk/cartopy/docs/v0.15/index.html#) library. See an example [here](http://scitools.org.uk/cartopy/docs/v0.15/examples/tissot.html)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
