{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# The Python ecosystem - The scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Scikit-learn is probably the most used Machine Learning Library in Python. It is built on NumPy, SciPy, and matplotlib. The library offers a simple and efficient tools for data mining and data analysis. Scikit-learn offers a consistent API across different model and applications and hence is one of the best tools in Python for shallow learning algorithms.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling pipeline in scikit learn \n",
    "\n",
    "* Loading The Data\n",
    "* Training And Test Data\n",
    "* Preprocessing The Data\n",
    "* Create the Model\n",
    "* Model Fitting\n",
    "* Prediction\n",
    "* Evaluate the Model's Performance\n",
    "* Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the `src` directory as one where we can import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), 'src'))\n",
    "sys.path.append(src_dir)\n",
    "print(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Data\n",
    "\n",
    "Your data needs to be numeric and stored as NumPy arrays or SciPy sparse matrices. Other types that are convertible to numeric arrays, such as Pandas `DataFrame`, are also acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data for a Machine Learning model may look like this...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.random((10,5))\n",
    "X[X < 0.3] = 0\n",
    "print(\"Features array: {}\\n\".format(X.shape), X)\n",
    "y = np.array(['M','M','F','F','F','M','M','F','F','F'])\n",
    "print(\"Labels: {}\\n\".format(y.shape),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further scikit-learn provides a rich [dataset loading utilities](http://scikit-learn.org/stable/datasets/index.html). It comes with easy to load toy datasets, sample images and sample generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy datasets:**\n",
    "\n",
    "Boston house-prices dataset (regression)\n",
    "\n",
    "    load_boston() \t\n",
    "    \n",
    "Iris dataset (classification)\n",
    "\n",
    "    load_iris()\n",
    "\n",
    "Diabetes dataset (regression)\n",
    "\n",
    "    load_diabetes() \n",
    "\n",
    "Digits dataset (classification)\n",
    "\n",
    "    load_digits()\n",
    "\n",
    "Linnerud dataset (multivariate regression) \n",
    "\n",
    "    load_linnerud()\n",
    " \n",
    "Wine dataset (classification)\n",
    "  \n",
    "    load_wine() \t\n",
    "\n",
    "Breast cancer wisconsin dataset (classification)\n",
    "  \n",
    "    load_breast_cancer() \t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "ds = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features array: {}\\n\".format(ds.data.shape), ds.data)\n",
    "print(\"Labels: {}\\n\".format(ds.target.shape),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample generators:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=25, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, \n",
    "                           random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features array: {}\\n\".format(X.shape), X)\n",
    "print(\"Labels: {}\\n\".format(y.shape),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, \n",
    "                           weights=[0.096], \n",
    "                           random_state=33)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,  \n",
    "                                                    shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set size:\" , X_train.shape, \"\\nTest set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train label size:\" , y_train.shape, \"\\nTest label size:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `stratify` parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter `stratify`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    test_size=0.4, random_state=42)\n",
    "\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, __learning algorithms benefit from standardization of the data set__. The `sklearn.preprocessing` package provides several utility functions and transformer classes to change raw feature vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train) \n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean: \",np.mean(X_train_scaled, axis=0))\n",
    "print(\"\\nStandard deviation: \", np.std(X_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative standardization is __aling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = min_max_scaler.transform(X_train) \n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min: \", np.min(X_train_scaled, axis=0))\n",
    "print(\"\\nMax: \", np.max(X_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encoding categorical features\n",
    "\n",
    "Often features are not given as continuous values but categorical. For example a person could have features `\"male\"`, `\"female\"` or `\"high\"`, `\"low\"`, `\"medium\"`. Such features can be efficiently coded as integers, such as `0`, `1` (_nominal_)  or `2`, `0`, `1` (_ordinal_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([['Male', 1, 0.76], ['Female', 3, 0.22], ['Female', 2, 0.57]])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(X_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform(X_train[:,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = np.copy(X_train)\n",
    "X_encoded[:,0] = le.transform(X_train[:,0])\n",
    "X_encoded = X_encoded.astype(float)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform(X_encoded[:,0].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded[:,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = preprocessing.OneHotEncoder()\n",
    "ohe.fit(X_encoded[:,0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.transform(X_encoded[:,0].reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_Note that starting with scikit-learn version 20.0 there will be a `CategoricalEncoder` available to convert categorical features to by __one-hot encoding__ or __ordinal encoding__ in one step_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many real world datasets contain missing values, often encoded as blanks, `NaN`s or other placeholders. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. Another strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
    "\n",
    "IN scikit-learn the `Imputer` class provides basic strategies for imputing missing values, either using __the mean__, __the median__ or __the most frequent value__ of the row or column in which the missing values are located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "Scikit-learn ships with many different supervised and unsupervised models as well as some basic neural network models; among those are:\n",
    "\n",
    "* ### Linear models\n",
    "  * Ordinary Least Squares\n",
    "  * Polynomial regression\n",
    "  * Ridge Regression\n",
    "  * Lasso\n",
    "  * Elastic Net\n",
    "  * Logistic regression\n",
    "\n",
    "* ### Support Vector Machines\n",
    "\n",
    "* ### Nearest Neighbors\n",
    "\n",
    "* ### Gaussian Processes\n",
    "\n",
    "* ### Decision Trees\n",
    "\n",
    "* ### Ensemble methods\n",
    "  * Random Forest\n",
    "  * Gradient Boosting\n",
    "\n",
    "Visit the [scikit-learn documentation website](http://scikit-learn.org/stable/index.html) for a comprehensive list of avaiake models and techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a toy data set\n",
    "\n",
    "For  the sake of this tutorial we load the _wine dataset_ provided by scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_wine()\n",
    "print(ds.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the data set to two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ds.target[ds.target < 2]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.data[ds.target < 2]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of a model instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial we introduce [**Logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression), also known as **logit regression**, or **logit model**, a probabilistic linear model for dichotomous data. The response variable is a binary variable (nominal variable), which means the variable has two categories or two values; Class 1 vs. Class 2, True vs. False, or $1$ vs. $0$, or success vs. failure, with the probabilities of $\\pi$ and $1-\\pi$, respectively. \n",
    "\n",
    "The output of a logistic regression is a probability $(\\pi)$, thus a value between $0$ and $1$. Moreover, this output is a linear function of known covariates $x_i$, which is just another word for the features in our data set. \n",
    "$$\\pi =\\beta_0+ \\beta_1x_1+ \\beta_2x_2+ ... +\\beta_kx_k$$\n",
    "\n",
    "However, the right term of the equation can take any real value, whereas the left term of the equation is a probability, on the scale $0$ to $1$. In order to transform the scale of the data (right term) into a probability between $0$ and $1$ we apply a so-called **link function**. \n",
    "\n",
    "For the logistic regression model this link function is the [**logit function**](https://en.wikipedia.org/wiki/Logit). The logit function maps probabilities from the range $(0, 1)$ to the entire real number range $(-\\infty, \\infty)$. It is written as \n",
    "\n",
    "$$\\eta = logit(\\pi)\\text{,}$$\n",
    "\n",
    "where $\\pi$ is the probability. \n",
    "\n",
    "To understand the logit we first introduce the [**odds ratio**](https://en.wikipedia.org/wiki/Odds_ratio) or in short **odds**. The odds (o) can be written as \n",
    "\n",
    "$$o = \\frac{\\pi}{1-\\pi}\\text{,}$$\n",
    "\n",
    "where $\\pi$ is the probability that an event occurs. If the probability of an event is a $0.5$, the odds are one-to-one or even $\\left(\\frac{0.5}{1-0.5}=1\\right)$. We further define the or **log-odds**, which is the logarithm of the odds:\n",
    "$$\\eta = logit(\\pi)= log \\left( \\frac{\\pi}{1-\\pi}\\right)$$\n",
    "\n",
    "This logarithmic function has the effect of removing the floor restriction, thus the function, the [**logit function**](https://en.wikipedia.org/wiki/Logit), our link function, transforms values in the range $0$ to $1$ to values over the entire real number range $(-\\infty, \\infty)$. If the probability is $1/2$ the odds are even and the logit is zero. Negative logits represent probabilities below one half and positive logits correspond to probabilities above one half.\n",
    "\n",
    "The inverse form of the logit function is also called the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), sometimes simply abbreviated as [**sigmoid function**](https://en.wikipedia.org/wiki/Sigmoid_function) due to its characteristic S-shape. Is allows us to go back from logits to probabilities.\n",
    "\n",
    "$$\\pi =logit^{-1}(\\eta)= \\frac{e^{\\eta}}{1+e^{\\eta}}=\\frac{1}{1+e^{-\\eta}}=\\frac{1}{1+e^{-\\beta_0+ \\beta_1x_1+ \\beta_2x_2+ ... +\\beta_kx_k}}$$\n",
    "\n",
    "The logistic function for the interval $[-6,6]$ is shown below. For values of $\\eta$ in the range from $-\\infty$ to $\\infty$ $\\pi$ is in the range of $0$ to $1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(interval=[-6,6]):\n",
    "    x = np.linspace(interval[0], interval[1])\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return (x,y)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(logit()[0], logit()[1])\n",
    "ax.axvline(0, color=\"k\", linestyle=\"dashed\", linewidth=0.5)\n",
    "ax.axhline(0.5, color=\"k\", linestyle=\"dashed\", linewidth=0.5)\n",
    "ax.set_title(\"The logistic function\", size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit function maps probabilities to values over the entire real number range. Thus, the probability of an event/outcome/success to be true $(y=1)$, given the set of predictors $x_i$, which is our data, is written as\n",
    "\n",
    "$$logit(P(y=1|x_i))= \\beta_0+ \\beta_1x_1+ \\beta_2x_2+ ... +\\beta_kx_k\\text{,}$$\n",
    "For a matter of simplification we express the inverse of the function above as \n",
    "\n",
    "$$\\phi(\\eta) = \\frac{1}{1+e^{-\\eta}}\\text{,}$$\n",
    "\n",
    "where $\\eta$ is the linear combination of coefficients $(\\beta_i)$ and predictor variables $(x_i)$, calculated as $\\eta = \\beta_0+ \\beta_1x_1+ \\beta_2x_2+ ... +\\beta_kx_k$.\n",
    "\n",
    "The parameters $(\\beta_i)$ of the logit model are estimated by the [**method of maximum likelihood**](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation). However, there is no closed-form solution, so the maximum likelihood estimates are obtained by using iterative algorithms such as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), among others. \n",
    "\n",
    "The output of the sigmoid function is interpreted as the probability of a particular observation belonging to class 1. It is written as  $\\phi(\\eta)=P(y=1|x_i,\\beta_i)$, the probability of success $(y=1)$ given the predictor variables $x_i$ parameterized by the coefficients $\\beta_i$. For example, if we compute $\\phi(\\eta)=0.65$ for a particular observation, this means that the chance that this observation belongs to class 1 is 65%. Similarly, the probability that this observation belongs to class 2 is calculated as $\\phi(\\eta)=P(y=0|x_i,\\beta_i)= 1 - P(y=1|x_i,\\beta_i)=1-0.65=0.35$ or 35%. For class assignment the predicted probability is then converted into a binary outcome via a unit step function:\n",
    "\n",
    "$$\n",
    "\\hat y =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $\\phi(\\eta) \\ge$ 0.5} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict_proba(X_test)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluate the Model's Performance\n",
    "\n",
    "In scikit-learn there are 3 different APIs for evaluating the quality of a model’s predictions:\n",
    "\n",
    "* __Estimator score method__: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. \n",
    "* __Scoring parameter__: Model-evaluation tools using cross-validation rely on an internal scoring strategy. \n",
    "* __Metric functions__: The metrics module implements functions assessing prediction error for specific purposes. Functions ending with `_score` return a value to maximize, the higher the better, and Functions ending with `_error` or `_loss` return a value to minimize, the lower the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator score method\n",
    "\n",
    "Returns the mean accuracy on the given test data and labels.\n",
    "\n",
    "$$\\text{accuracy}(y, \\hat y) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}\\mathbb1(\\hat y_i = y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are many more [model metrics](http://scikit-learn.org/stable/modules/model_evaluation.html) available in scikit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(sklearn.metrics)[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that there are two types of parameters: \n",
    "* __model parameters__ and \n",
    "* __hyperparameters__. \n",
    "\n",
    "Models parameters (i.e. regression coefficients) are learned from the data. Hyperparameters however, are parameters whose values are set before the learning process begins. Different model training algorithms require different hyperparameters, some simple algorithms  require none. \n",
    "\n",
    "The optimal hyperparameter configuration for a particular modeling task is unknown. Hence, we apply different techniques, such as grid search, random search or Baysian optimization to approximate the best hyperparamter configuration (referred to as [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
    "\n",
    "During the process of model optimization we want to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting). Hence, in machine learning a technique referred to as [k-fold cross-validation][1] is applied. Cross-validation is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.\n",
    "\n",
    "#### K-fold cross-validation (Source: [Wikipedia][1])\n",
    "![](./_img/K-fold_cross_validation_EN.png)\n",
    "\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Cross-validation_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutor ail we apply [support vector machines]() (SVMs) for classification of the subseted _wine data set_ (see above). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. \n",
    "\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. \n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates.\n",
    "* SVM do not scale well.\n",
    "\n",
    "The mathematics of the algorithm are beyond the scope of this tutorial, however if you are interested we suggest to dive into the [scikit-learn documentation](http://scikit-learn.org/stable/modules/svm.html) or watch the informative [video](https://www.youtube.com/watch?v=-Z4aojJ-pdg) by Brandon Rohrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `C`: Penalty parameter `C` of the error term. The parameter allows one to trade off training error vs. model complexity. Hence, the `C` parameter trades off misclassification of training examples against simplicity of the decision surface. A low `C` makes the decision surface smooth, while a high `C` aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.\n",
    "\n",
    "* `kernel`:  Specifies the kernel type to be used in the algorithm. It must be one of `linear`, `poly`, `rbf`, `sigmoid`, `precomputed` or a callable. \n",
    "\n",
    "* `gamma`: Kernel coefficient for `rbf`, `poly` and `sigmoid`. Intuitively, the `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning _far_ and high values meaning _close_. The `gamma` parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'C': [0.1, 1, 10, 100],\n",
    "               'gamma': [0.01, 0.001, 0.0001],\n",
    "               'kernel': ['linear', 'rbf']}]\n",
    "\n",
    "clf = GridSearchCV(svc, param_grid, cv=5, verbose=1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`best_params_`: Parameter setting that gave the best results on the hold out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`best_estimator_`: Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`best_score_`: Mean cross-validated score of the best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preditction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.best_estimator_.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "hf.plot_confusion_matrix(cnf_matrix, classes=[\"class 0\", \"class 1\"],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seclected classification metrics**\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{precision} = \\frac{TP}{TP+FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{recall} = \\frac{TP}{TP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{F1-score} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
